{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141cfb36",
   "metadata": {},
   "source": [
    "# Part A: Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "58b9f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878bd6",
   "metadata": {},
   "source": [
    "# Part B: Gradiant Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1d6c7fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss:3517725.5, grad_norm: 5721.951171875\n",
      "epoch: 1, loss:80558546944.0, grad_norm: 70389.296875\n",
      "epoch: 2, loss:1844851662389248.0, grad_norm: 865902.6875\n",
      "epoch: 3, loss:4.224850999692073e+19, grad_norm: 10652010.0\n",
      "epoch: 4, loss:9.675229356395934e+23, grad_norm: 131037024.0\n",
      "epoch: 5, loss:2.2157012971970574e+28, grad_norm: 1611968384.0\n",
      "epoch: 6, loss:5.07412356661216e+32, grad_norm: inf\n",
      "epoch: 7, loss:inf, grad_norm: inf\n",
      "epoch: 8, loss:inf, grad_norm: inf\n",
      "epoch: 9, loss:inf, grad_norm: inf\n",
      "epoch: 10, loss:inf, grad_norm: inf\n",
      "epoch: 11, loss:inf, grad_norm: inf\n",
      "epoch: 12, loss:inf, grad_norm: inf\n",
      "epoch: 13, loss:inf, grad_norm: inf\n",
      "epoch: 14, loss:inf, grad_norm: inf\n",
      "epoch: 15, loss:inf, grad_norm: inf\n",
      "epoch: 16, loss:inf, grad_norm: nan\n",
      "epoch: 17, loss:nan, grad_norm: nan\n",
      "epoch: 18, loss:nan, grad_norm: nan\n",
      "epoch: 19, loss:nan, grad_norm: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingv\\AppData\\Local\\Temp\\ipykernel_15732\\2001907731.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(torch.ones((1000, 1)), dtype=torch.float32).to('cuda')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_grad_norm(parameters):\n",
    "    total_norm = 0.0\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            total_norm += p.grad.norm(2)\n",
    "    \n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm.item()\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(\"../data/BTCUSDT.csv\", sep='|', nrows=1000, header=None, usecols=[1, 2, 3, 4, 5],\n",
    "                       names=['open', 'high', 'low', 'close', 'volume'])\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32).to('cuda')\n",
    "target = torch.tensor(torch.ones((1000, 1)), dtype=torch.float32).to('cuda') \n",
    "\n",
    "\n",
    "model = nn.Linear(5, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.000001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\")\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_tensor, target)\n",
    "    print(f\"epoch: {epoch}, loss:{loss}, grad_norm: {get_grad_norm(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "79f0be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6dd52633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:894990.375,\t grad_norm: 1.000\n",
      "epoch: 1,\t loss:737480.500,\t grad_norm: 1.000\n",
      "epoch: 2,\t loss:595203.625,\t grad_norm: 1.000\n",
      "epoch: 3,\t loss:468159.656,\t grad_norm: 1.000\n",
      "epoch: 4,\t loss:356348.719,\t grad_norm: 1.000\n",
      "epoch: 5,\t loss:259770.812,\t grad_norm: 1.000\n",
      "epoch: 6,\t loss:178425.859,\t grad_norm: 1.000\n",
      "epoch: 7,\t loss:112313.922,\t grad_norm: 1.000\n",
      "epoch: 8,\t loss:61435.051,\t grad_norm: 1.000\n",
      "epoch: 9,\t loss:25789.127,\t grad_norm: 1.000\n",
      "epoch: 10,\t loss:5376.165,\t grad_norm: 1.000\n",
      "epoch: 11,\t loss:196.212,\t grad_norm: 1.000\n",
      "epoch: 12,\t loss:5376.155,\t grad_norm: 1.000\n",
      "epoch: 13,\t loss:196.214,\t grad_norm: 1.000\n",
      "epoch: 14,\t loss:5376.146,\t grad_norm: 1.000\n",
      "epoch: 15,\t loss:196.216,\t grad_norm: 1.000\n",
      "epoch: 16,\t loss:5376.135,\t grad_norm: 1.000\n",
      "epoch: 17,\t loss:196.218,\t grad_norm: 1.000\n",
      "epoch: 18,\t loss:5376.126,\t grad_norm: 1.000\n",
      "epoch: 19,\t loss:196.219,\t grad_norm: 1.000\n"
     ]
    }
   ],
   "source": [
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_tensor, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.3f},\\t grad_norm: {get_grad_norm(model.parameters()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a250a5",
   "metadata": {},
   "source": [
    "# Part C: Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4a7e52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.scaler = torch.amp.GradScaler()\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=self.device, dtype=torch.float16):\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "dd83b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 1,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 2,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 3,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 4,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 5,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 6,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 7,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 8,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 9,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 10,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 11,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 12,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 13,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 14,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 15,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 16,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 17,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 18,\t loss:14492333.000,\t grad_norm: nan\n",
      "epoch: 19,\t loss:14492333.000,\t grad_norm: nan\n"
     ]
    }
   ],
   "source": [
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_tensor, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.3f},\\t grad_norm: {get_grad_norm(model.parameters()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eae007",
   "metadata": {},
   "source": [
    "# Part D: AMP Integration with Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4b950fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:3.21218,\t grad_norm: nan,\t scale: 32768.0\n",
      "epoch: 1,\t loss:3.21218,\t grad_norm: nan,\t scale: 16384.0\n",
      "epoch: 2,\t loss:3.21218,\t grad_norm: 1.1789,\t scale: 16384.0\n",
      "epoch: 3,\t loss:2.31021,\t grad_norm: 1.1886,\t scale: 16384.0\n",
      "epoch: 4,\t loss:1.62655,\t grad_norm: 1.1818,\t scale: 16384.0\n",
      "epoch: 5,\t loss:1.11609,\t grad_norm: 1.1386,\t scale: 16384.0\n",
      "epoch: 6,\t loss:0.72702,\t grad_norm: 1.0684,\t scale: 16384.0\n",
      "epoch: 7,\t loss:0.42569,\t grad_norm: 1.0387,\t scale: 16384.0\n",
      "epoch: 8,\t loss:0.20506,\t grad_norm: 0.9881,\t scale: 16384.0\n",
      "epoch: 9,\t loss:0.07380,\t grad_norm: 0.7652,\t scale: 16384.0\n",
      "epoch: 10,\t loss:0.02659,\t grad_norm: 0.5925,\t scale: 16384.0\n",
      "epoch: 11,\t loss:0.00973,\t grad_norm: 0.4599,\t scale: 16384.0\n",
      "epoch: 12,\t loss:0.00359,\t grad_norm: 0.3565,\t scale: 16384.0\n",
      "epoch: 13,\t loss:0.00138,\t grad_norm: 0.2764,\t scale: 16384.0\n",
      "epoch: 14,\t loss:0.00058,\t grad_norm: 0.2142,\t scale: 16384.0\n",
      "epoch: 15,\t loss:0.00028,\t grad_norm: 0.1645,\t scale: 16384.0\n",
      "epoch: 16,\t loss:0.00019,\t grad_norm: 0.1330,\t scale: 16384.0\n",
      "epoch: 17,\t loss:0.00015,\t grad_norm: 0.1028,\t scale: 16384.0\n",
      "epoch: 18,\t loss:0.00013,\t grad_norm: 0.0807,\t scale: 16384.0\n",
      "epoch: 19,\t loss:0.00013,\t grad_norm: 0.0671,\t scale: 16384.0\n"
     ]
    }
   ],
   "source": [
    "data_norm = ((data_tensor - data_tensor.mean(dim=0)) / data_tensor.std(dim=0)).to(\"cuda\")\n",
    "\n",
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_norm, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.5f},\\t grad_norm: {get_grad_norm(model.parameters()):.4f},\\t scale: {trainer.scaler.get_scale()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7f984",
   "metadata": {},
   "source": [
    "# Part E: Checkpointing System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "90f6bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.scaler = torch.amp.GradScaler()\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=self.device, dtype=torch.float16):\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()\n",
    "    \n",
    "    def save_checkpoint(self, filepath):\n",
    "        state = dict()\n",
    "        state['model_state'] = self.model.state_dict()\n",
    "        state['optimizer_state'] = self.optimizer.state_dict()\n",
    "        state['scaler_state'] = self.scaler.state_dict()\n",
    "        torch.save(state, filepath)\n",
    "\n",
    "    def load_checkpoint(self, filepath):\n",
    "        state = torch.load(filepath, weights_only=True)\n",
    "        self.model.load_state_dict(state['model_state'])\n",
    "        self.optimizer.load_state_dict(state['optimizer_state'])\n",
    "        self.scaler.load_state_dict(state['scaler_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4b4c0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:1.96366,\t grad_norm: nan,\t scale: 32768.0\n",
      "epoch: 1,\t loss:1.96366,\t grad_norm: nan,\t scale: 16384.0\n",
      "epoch: 2,\t loss:1.96366,\t grad_norm: 1.1882,\t scale: 16384.0\n",
      "epoch: 3,\t loss:1.37341,\t grad_norm: 1.1647,\t scale: 16384.0\n",
      "epoch: 4,\t loss:0.92736,\t grad_norm: 1.1062,\t scale: 16384.0\n",
      "epoch: 5,\t loss:0.58079,\t grad_norm: 1.0607,\t scale: 16384.0\n",
      "epoch: 6,\t loss:0.31622,\t grad_norm: 1.0559,\t scale: 16384.0\n",
      "epoch: 7,\t loss:0.13184,\t grad_norm: 0.8981,\t scale: 16384.0\n",
      "epoch: 8,\t loss:0.04815,\t grad_norm: 0.6959,\t scale: 16384.0\n",
      "epoch: 9,\t loss:0.01793,\t grad_norm: 0.5388,\t scale: 16384.0\n",
      "epoch: 10,\t loss:0.00717,\t grad_norm: 0.4185,\t scale: 16384.0\n",
      "epoch: 11,\t loss:0.00324,\t grad_norm: 0.3245,\t scale: 16384.0\n",
      "epoch: 12,\t loss:0.00183,\t grad_norm: 0.2530,\t scale: 16384.0\n",
      "epoch: 13,\t loss:0.00132,\t grad_norm: 0.1985,\t scale: 16384.0\n",
      "epoch: 14,\t loss:0.00114,\t grad_norm: 0.1600,\t scale: 16384.0\n",
      "epoch: 15,\t loss:0.00107,\t grad_norm: 0.1282,\t scale: 16384.0\n",
      "epoch: 16,\t loss:0.00105,\t grad_norm: 0.1110,\t scale: 16384.0\n",
      "epoch: 17,\t loss:0.00103,\t grad_norm: 0.0918,\t scale: 16384.0\n",
      "epoch: 18,\t loss:0.00102,\t grad_norm: 0.0860,\t scale: 16384.0\n",
      "epoch: 19,\t loss:0.00102,\t grad_norm: 0.0799,\t scale: 16384.0\n"
     ]
    }
   ],
   "source": [
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_norm, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.5f},\\t grad_norm: {get_grad_norm(model.parameters()):.4f},\\t scale: {trainer.scaler.get_scale()}\")\n",
    "\n",
    "trainer.save_checkpoint('test_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f41cd9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010149055160582066"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.load_checkpoint('test_model.pth')\n",
    "trainer.eval_step(data_norm, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
