{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141cfb36",
   "metadata": {},
   "source": [
    "# Part A: Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58b9f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878bd6",
   "metadata": {},
   "source": [
    "# Part B: Gradiant Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d6c7fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss:1279875.75, grad_norm: 4443.95849609375\n",
      "epoch: 1, loss:29309939712.0, grad_norm: 54667.90625\n",
      "epoch: 2, loss:671219703611392.0, grad_norm: 672504.0625\n",
      "epoch: 3, loss:1.5371443036168913e+19, grad_norm: 8272892.0\n",
      "epoch: 4, loss:3.5201763189153054e+23, grad_norm: 101770016.0\n",
      "epoch: 5, loss:8.061471542676563e+27, grad_norm: 1251936512.0\n",
      "epoch: 6, loss:1.8461382397331074e+32, grad_norm: inf\n",
      "epoch: 7, loss:inf, grad_norm: inf\n",
      "epoch: 8, loss:inf, grad_norm: inf\n",
      "epoch: 9, loss:inf, grad_norm: inf\n",
      "epoch: 10, loss:inf, grad_norm: inf\n",
      "epoch: 11, loss:inf, grad_norm: inf\n",
      "epoch: 12, loss:inf, grad_norm: inf\n",
      "epoch: 13, loss:inf, grad_norm: inf\n",
      "epoch: 14, loss:inf, grad_norm: inf\n",
      "epoch: 15, loss:inf, grad_norm: inf\n",
      "epoch: 16, loss:inf, grad_norm: nan\n",
      "epoch: 17, loss:nan, grad_norm: nan\n",
      "epoch: 18, loss:nan, grad_norm: nan\n",
      "epoch: 19, loss:nan, grad_norm: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingv\\AppData\\Local\\Temp\\ipykernel_15732\\2001907731.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(torch.ones((1000, 1)), dtype=torch.float32).to('cuda')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_grad_norm(parameters):\n",
    "    total_norm = 0.0\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            total_norm += p.grad.norm(2)\n",
    "    \n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm.item()\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(\"../data/BTCUSDT.csv\", sep='|', nrows=1000, header=None, usecols=[1, 2, 3, 4, 5],\n",
    "                       names=['open', 'high', 'low', 'close', 'volume'])\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32).to('cuda')\n",
    "target = torch.tensor(torch.ones((1000, 1)), dtype=torch.float32).to('cuda') \n",
    "\n",
    "\n",
    "model = nn.Linear(5, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.000001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\")\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_tensor, target)\n",
    "    print(f\"epoch: {epoch}, loss:{loss}, grad_norm: {get_grad_norm(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dd52633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:183512.297,\t grad_norm: 1.000\n",
      "epoch: 1,\t loss:116358.844,\t grad_norm: 1.000\n",
      "epoch: 2,\t loss:64438.531,\t grad_norm: 1.000\n",
      "epoch: 3,\t loss:27751.252,\t grad_norm: 1.000\n",
      "epoch: 4,\t loss:6296.945,\t grad_norm: 1.000\n",
      "epoch: 5,\t loss:75.647,\t grad_norm: 1.000\n",
      "epoch: 6,\t loss:6296.944,\t grad_norm: 1.000\n",
      "epoch: 7,\t loss:75.647,\t grad_norm: 1.000\n",
      "epoch: 8,\t loss:6296.944,\t grad_norm: 1.000\n",
      "epoch: 9,\t loss:75.648,\t grad_norm: 1.000\n",
      "epoch: 10,\t loss:6296.943,\t grad_norm: 1.000\n",
      "epoch: 11,\t loss:75.648,\t grad_norm: 1.000\n",
      "epoch: 12,\t loss:6296.945,\t grad_norm: 1.000\n",
      "epoch: 13,\t loss:75.647,\t grad_norm: 1.000\n",
      "epoch: 14,\t loss:6296.945,\t grad_norm: 1.000\n",
      "epoch: 15,\t loss:75.647,\t grad_norm: 1.000\n",
      "epoch: 16,\t loss:6296.944,\t grad_norm: 1.000\n",
      "epoch: 17,\t loss:75.647,\t grad_norm: 1.000\n",
      "epoch: 18,\t loss:6296.940,\t grad_norm: 1.000\n",
      "epoch: 19,\t loss:75.647,\t grad_norm: 1.000\n"
     ]
    }
   ],
   "source": [
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_tensor, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.3f},\\t grad_norm: {get_grad_norm(model.parameters()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a250a5",
   "metadata": {},
   "source": [
    "# Part C: Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.scaler = torch.amp.GradScaler()\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=self.device, dtype=torch.float16):\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        return loss.item()\n",
    "\n",
    "    def eval_step(self, x, y):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(y_pred, y)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dd83b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 1,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 2,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 3,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 4,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 5,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 6,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 7,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 8,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 9,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 10,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 11,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 12,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 13,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 14,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 15,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 16,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 17,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 18,\t loss:1166346.125,\t grad_norm: nan\n",
      "epoch: 19,\t loss:1166346.125,\t grad_norm: nan\n"
     ]
    }
   ],
   "source": [
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_tensor, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.3f},\\t grad_norm: {get_grad_norm(model.parameters()):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eae007",
   "metadata": {},
   "source": [
    "# Part D: AMP Integration with Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4b950fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,\t loss:2.09069,\t grad_norm: nan,\t scale: 32768.0\n",
      "epoch: 1,\t loss:2.09069,\t grad_norm: nan,\t scale: 16384.0\n",
      "epoch: 2,\t loss:2.09069,\t grad_norm: 1.1570,\t scale: 16384.0\n",
      "epoch: 3,\t loss:1.53029,\t grad_norm: 1.1148,\t scale: 16384.0\n",
      "epoch: 4,\t loss:1.07081,\t grad_norm: 1.0763,\t scale: 16384.0\n",
      "epoch: 5,\t loss:0.69637,\t grad_norm: 1.0612,\t scale: 16384.0\n",
      "epoch: 6,\t loss:0.40266,\t grad_norm: 1.0597,\t scale: 16384.0\n",
      "epoch: 7,\t loss:0.18890,\t grad_norm: 0.9879,\t scale: 16384.0\n",
      "epoch: 8,\t loss:0.06804,\t grad_norm: 0.7653,\t scale: 16384.0\n",
      "epoch: 9,\t loss:0.02467,\t grad_norm: 0.5935,\t scale: 16384.0\n",
      "epoch: 10,\t loss:0.00888,\t grad_norm: 0.4589,\t scale: 16384.0\n",
      "epoch: 11,\t loss:0.00328,\t grad_norm: 0.3557,\t scale: 16384.0\n",
      "epoch: 12,\t loss:0.00128,\t grad_norm: 0.2768,\t scale: 16384.0\n",
      "epoch: 13,\t loss:0.00053,\t grad_norm: 0.2138,\t scale: 16384.0\n",
      "epoch: 14,\t loss:0.00027,\t grad_norm: 0.1676,\t scale: 16384.0\n",
      "epoch: 15,\t loss:0.00017,\t grad_norm: 0.1308,\t scale: 16384.0\n",
      "epoch: 16,\t loss:0.00014,\t grad_norm: 0.1038,\t scale: 16384.0\n",
      "epoch: 17,\t loss:0.00012,\t grad_norm: 0.0815,\t scale: 16384.0\n",
      "epoch: 18,\t loss:0.00012,\t grad_norm: 0.0678,\t scale: 16384.0\n",
      "epoch: 19,\t loss:0.00012,\t grad_norm: 0.0589,\t scale: 16384.0\n"
     ]
    }
   ],
   "source": [
    "data_norm = ((data_tensor - data_tensor.mean(dim=0)) / data_tensor.std(dim=0)).to(\"cuda\")\n",
    "\n",
    "model.reset_parameters()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, \"cuda\", max_grad_norm=1)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = trainer.train_step(data_norm, target)\n",
    "    print(f\"epoch: {epoch},\\t loss:{loss:.5f},\\t grad_norm: {get_grad_norm(model.parameters()):.4f},\\t scale: {trainer.scaler.get_scale()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
