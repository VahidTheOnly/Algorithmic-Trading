{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf4c1c1",
   "metadata": {},
   "source": [
    "# Part A: Leaf Node and In-place Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "8b75e6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a leaf Variable that requires grad is being used in an in-place operation.\n",
      "grad: tensor([3., 2.])\n",
      "w: tensor([2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "output = w[0] * w[1]\n",
    "\n",
    "try:\n",
    "    w += 1.0  \n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "output.backward()\n",
    "\n",
    "print(f\"grad: {w.grad}\")\n",
    "print(f\"w: {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "6d341f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingv\\AppData\\Local\\Temp\\ipykernel_11428\\575195260.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "output = w[0] * w[1]\n",
    "\n",
    "w = w + 1.0\n",
    "\n",
    "output.backward()\n",
    "\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "4277380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([3., 2.])\n",
      "Updated w: tensor([3., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "output = w[0] * w[1]\n",
    "\n",
    "output.backward()\n",
    "print(\"Gradient:\", w.grad)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    w += 1.0          \n",
    "\n",
    "print(\"Updated w:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ffca7",
   "metadata": {},
   "source": [
    "# Part B: (mini-project) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "319570a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "b98dc628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       close\n",
      "timestamp                   \n",
      "2017-08-17 04:00:00  4261.48\n",
      "2017-08-17 04:01:00  4261.48\n",
      "2017-08-17 04:02:00  4280.56\n",
      "2017-08-17 04:03:00  4261.48\n",
      "2017-08-17 04:04:00  4261.48\n",
      "2017-08-17 04:05:00  4261.48\n",
      "2017-08-17 04:06:00  4261.48\n",
      "2017-08-17 04:07:00  4261.48\n",
      "2017-08-17 04:08:00  4261.48\n",
      "2017-08-17 04:09:00  4261.48\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"../data/BTCUSDT.csv\", sep='|', nrows=10, header=None, usecols=[0,4],\n",
    "                       names=['timestamp', 'close'])\n",
    "data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], unit='s')\n",
    "data_df.set_index('timestamp', inplace=True)\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "46234385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4261.4800],\n",
       "         [4261.4800],\n",
       "         [4280.5601],\n",
       "         [4261.4800],\n",
       "         [4261.4800],\n",
       "         [4261.4800],\n",
       "         [4261.4800],\n",
       "         [4261.4800],\n",
       "         [4261.4800],\n",
       "         [4261.4800]]),\n",
       " torch.Size([10, 1]))"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "data_tensor, data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "1ed4f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: tensor([[1.0877e+08]], grad_fn=<PowBackward0>)\n",
      "Epoch: 1, loss: tensor([[44060300.]], grad_fn=<PowBackward0>)\n",
      "Epoch: 2, loss: tensor([[17848558.]], grad_fn=<PowBackward0>)\n",
      "Epoch: 3, loss: tensor([[7230337.5000]], grad_fn=<PowBackward0>)\n",
      "Epoch: 4, loss: tensor([[2928966.5000]], grad_fn=<PowBackward0>)\n",
      "Epoch: 5, loss: tensor([[1186504.8750]], grad_fn=<PowBackward0>)\n",
      "Epoch: 6, loss: tensor([[480645.6562]], grad_fn=<PowBackward0>)\n",
      "Epoch: 7, loss: tensor([[194706.7344]], grad_fn=<PowBackward0>)\n",
      "Epoch: 8, loss: tensor([[78874.5859]], grad_fn=<PowBackward0>)\n",
      "Epoch: 9, loss: tensor([[31951.5625]], grad_fn=<PowBackward0>)\n",
      "Epoch: 10, loss: tensor([[12943.3955]], grad_fn=<PowBackward0>)\n",
      "Epoch: 11, loss: tensor([[5243.1602]], grad_fn=<PowBackward0>)\n",
      "Epoch: 12, loss: tensor([[2124.0037]], grad_fn=<PowBackward0>)\n",
      "Epoch: 13, loss: tensor([[860.3967]], grad_fn=<PowBackward0>)\n",
      "Epoch: 14, loss: tensor([[348.5295]], grad_fn=<PowBackward0>)\n",
      "Epoch: 15, loss: tensor([[141.2012]], grad_fn=<PowBackward0>)\n",
      "Epoch: 16, loss: tensor([[57.1988]], grad_fn=<PowBackward0>)\n",
      "Epoch: 17, loss: tensor([[23.1696]], grad_fn=<PowBackward0>)\n",
      "Epoch: 18, loss: tensor([[9.3849]], grad_fn=<PowBackward0>)\n",
      "Epoch: 19, loss: tensor([[3.7976]], grad_fn=<PowBackward0>)\n",
      "weights after training :  tensor([[ 0.0057],\n",
      "        [ 0.2774],\n",
      "        [ 0.2360],\n",
      "        [-2.7043],\n",
      "        [-0.2539],\n",
      "        [ 0.8174],\n",
      "        [ 0.2296],\n",
      "        [ 1.1674],\n",
      "        [-0.1079],\n",
      "        [ 0.3551]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn_like(data_tensor, requires_grad=True)\n",
    "\n",
    "target = torch.tensor(100)\n",
    "\n",
    "learning_rate = 1e-9\n",
    "\n",
    "for i in range(20):\n",
    "    y = w.T @ data_tensor\n",
    "\n",
    "    loss = (y - target) ** 2\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"Epoch: {i}, loss: {loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(\"weights after training : \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e89a78",
   "metadata": {},
   "source": [
    "# Part C: Advanced Indexing and Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "20aa7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.2615e+03, 4.2615e+03, 4.2615e+03, 4.2615e+03, 1.7752e+00],\n",
      "        [4.2615e+03, 4.2615e+03, 4.2615e+03, 4.2615e+03, 0.0000e+00],\n",
      "        [4.2806e+03, 4.2806e+03, 4.2806e+03, 4.2806e+03, 2.6107e-01],\n",
      "        [4.2615e+03, 4.2615e+03, 4.2615e+03, 4.2615e+03, 1.2008e-02],\n",
      "        [4.2615e+03, 4.2615e+03, 4.2615e+03, 4.2615e+03, 1.4080e-01]])\n"
     ]
    }
   ],
   "source": [
    "data_df2 = pd.read_csv(\"../data/BTCUSDT.csv\", sep='|', nrows=1000, header=None, usecols=[1, 2, 3, 4, 5],\n",
    "                       names=['open', 'high', 'low', 'close', 'volume'])\n",
    "data_tensor2 = torch.tensor(data_df2.values, dtype=torch.float32)\n",
    "print(data_tensor2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "dc50ff69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high = data_tensor2[:, 1]\n",
    "low = data_tensor2[:, 2]\n",
    "close = data_tensor2[:, 3]\n",
    "\n",
    "typical_price = (high + low + close) / 3\n",
    "typical_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "3536c9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume = data_tensor2[:, -1]\n",
    "min_vol = torch.min(volume)\n",
    "vol_normalized = (volume - min_vol) / (torch.max(volume) - min_vol)\n",
    "vol_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "d6cd3d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110, 5])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bullish = data_tensor2[data_tensor2[:, 3] > data_tensor2[:, 0]]\n",
    "bullish.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55a2a6",
   "metadata": {},
   "source": [
    "# Part D: Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "e8aa152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearRewardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, 5))\n",
    "        self.register_buffer('profit_target', torch.tensor([[100.0]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.weights.T \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "1bd79dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "6422face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 60522124.0\n",
      "epoch: 1, loss: 16573596.0\n",
      "epoch: 2, loss: 4538615.0\n",
      "epoch: 3, loss: 1242926.625\n",
      "epoch: 4, loss: 340426.96875\n",
      "epoch: 5, loss: 93284.265625\n",
      "epoch: 6, loss: 25606.3203125\n",
      "epoch: 7, loss: 7073.2080078125\n",
      "epoch: 8, loss: 1998.0361328125\n",
      "epoch: 9, loss: 608.2025146484375\n",
      "epoch: 10, loss: 227.62669372558594\n",
      "epoch: 11, loss: 123.40994262695312\n",
      "epoch: 12, loss: 94.86837768554688\n",
      "epoch: 13, loss: 87.05130004882812\n",
      "epoch: 14, loss: 84.91140747070312\n",
      "epoch: 15, loss: 84.32617950439453\n",
      "epoch: 16, loss: 84.16561126708984\n",
      "epoch: 17, loss: 84.12134552001953\n",
      "epoch: 18, loss: 84.1092300415039\n",
      "epoch: 19, loss: 84.10565948486328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingv\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "data_tensor2 = data_tensor2.to(device)\n",
    "model = LinearRewardNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(data_tensor2)\n",
    "    loss = criterion(y_pred, model.profit_target)\n",
    "    print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "b77e95ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[98.0205],\n",
      "        [95.2207],\n",
      "        [96.0581],\n",
      "        [95.2397],\n",
      "        [95.4429]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict = model(data_tensor2)\n",
    "\n",
    "    print(predict[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca2b9a",
   "metadata": {},
   "source": [
    "# Part E: Normalizing and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "0b5b7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = ((data_tensor2 - data_tensor2.mean(dim=0)) / data_tensor2.std(dim=0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "da59dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRewardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, 5))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.register_buffer('profit_target', torch.tensor([[100.0]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.weights.T + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "fb61e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 10008.4619140625\n",
      "epoch: 1, loss: 9512.1865234375\n",
      "epoch: 2, loss: 6499.46630859375\n",
      "epoch: 3, loss: 5235.42626953125\n",
      "epoch: 4, loss: 4450.0234375\n",
      "epoch: 5, loss: 3137.924072265625\n",
      "epoch: 6, loss: 1838.6861572265625\n",
      "epoch: 7, loss: 1059.1693115234375\n",
      "epoch: 8, loss: 791.76513671875\n",
      "epoch: 9, loss: 644.3317260742188\n",
      "epoch: 10, loss: 384.6661682128906\n",
      "epoch: 11, loss: 117.71128845214844\n",
      "epoch: 12, loss: 36.45444107055664\n",
      "epoch: 13, loss: 193.6685333251953\n",
      "epoch: 14, loss: 451.08465576171875\n",
      "epoch: 15, loss: 628.8782958984375\n",
      "epoch: 16, loss: 678.1686401367188\n",
      "epoch: 17, loss: 678.7376098632812\n",
      "epoch: 18, loss: 719.578369140625\n",
      "epoch: 19, loss: 803.1793212890625\n",
      "epoch: 20, loss: 852.3026733398438\n",
      "epoch: 21, loss: 802.9795532226562\n",
      "epoch: 22, loss: 672.394775390625\n",
      "epoch: 23, loss: 532.4863891601562\n",
      "epoch: 24, loss: 435.94793701171875\n",
      "epoch: 25, loss: 374.0062255859375\n",
      "epoch: 26, loss: 302.89031982421875\n",
      "epoch: 27, loss: 203.14093017578125\n",
      "epoch: 28, loss: 101.83050537109375\n",
      "epoch: 29, loss: 39.6816520690918\n",
      "epoch: 30, loss: 27.273223876953125\n",
      "epoch: 31, loss: 37.99573516845703\n",
      "epoch: 32, loss: 41.288665771484375\n",
      "epoch: 33, loss: 34.316566467285156\n",
      "epoch: 34, loss: 36.79743957519531\n",
      "epoch: 35, loss: 61.069454193115234\n",
      "epoch: 36, loss: 95.03874206542969\n",
      "epoch: 37, loss: 116.09626770019531\n",
      "epoch: 38, loss: 116.22727966308594\n",
      "epoch: 39, loss: 107.24034118652344\n",
      "epoch: 40, loss: 102.865478515625\n",
      "epoch: 41, loss: 102.2346420288086\n",
      "epoch: 42, loss: 94.12623596191406\n",
      "epoch: 43, loss: 73.84503173828125\n",
      "epoch: 44, loss: 49.89581298828125\n",
      "epoch: 45, loss: 33.34501647949219\n",
      "epoch: 46, loss: 25.392541885375977\n",
      "epoch: 47, loss: 18.819812774658203\n",
      "epoch: 48, loss: 9.465513229370117\n",
      "epoch: 49, loss: 1.512887716293335\n",
      "epoch: 50, loss: 0.6521454453468323\n",
      "epoch: 51, loss: 5.880030632019043\n",
      "epoch: 52, loss: 11.025473594665527\n",
      "epoch: 53, loss: 12.752547264099121\n",
      "epoch: 54, loss: 13.532285690307617\n",
      "epoch: 55, loss: 16.442516326904297\n",
      "epoch: 56, loss: 20.158723831176758\n",
      "epoch: 57, loss: 21.016155242919922\n",
      "epoch: 58, loss: 18.27803611755371\n",
      "epoch: 59, loss: 14.777912139892578\n",
      "epoch: 60, loss: 12.674215316772461\n",
      "epoch: 61, loss: 10.958170890808105\n",
      "epoch: 62, loss: 7.906205177307129\n",
      "epoch: 63, loss: 4.145675182342529\n",
      "epoch: 64, loss: 1.7607948780059814\n",
      "epoch: 65, loss: 1.2998665571212769\n",
      "epoch: 66, loss: 1.3523398637771606\n",
      "epoch: 67, loss: 0.9279533624649048\n",
      "epoch: 68, loss: 0.7129104137420654\n",
      "epoch: 69, loss: 1.5471875667572021\n",
      "epoch: 70, loss: 2.8792035579681396\n",
      "epoch: 71, loss: 3.5712263584136963\n",
      "epoch: 72, loss: 3.5039260387420654\n",
      "epoch: 73, loss: 3.432689666748047\n",
      "epoch: 74, loss: 3.6030616760253906\n",
      "epoch: 75, loss: 3.455052375793457\n",
      "epoch: 76, loss: 2.6903202533721924\n",
      "epoch: 77, loss: 1.8003159761428833\n",
      "epoch: 78, loss: 1.2878254652023315\n",
      "epoch: 79, loss: 0.9954363107681274\n",
      "epoch: 80, loss: 0.5712538957595825\n",
      "epoch: 81, loss: 0.13408085703849792\n",
      "epoch: 82, loss: 0.03255721554160118\n",
      "epoch: 83, loss: 0.2339046448469162\n",
      "epoch: 84, loss: 0.4016823470592499\n",
      "epoch: 85, loss: 0.4365292489528656\n",
      "epoch: 86, loss: 0.5308396220207214\n",
      "epoch: 87, loss: 0.7374841570854187\n",
      "epoch: 88, loss: 0.855649471282959\n",
      "epoch: 89, loss: 0.7784122824668884\n",
      "epoch: 90, loss: 0.6464328765869141\n",
      "epoch: 91, loss: 0.578349769115448\n",
      "epoch: 92, loss: 0.4980025887489319\n",
      "epoch: 93, loss: 0.3296635150909424\n",
      "epoch: 94, loss: 0.1607297956943512\n",
      "epoch: 95, loss: 0.09220138192176819\n",
      "epoch: 96, loss: 0.08202319592237473\n",
      "epoch: 97, loss: 0.0516832061111927\n",
      "epoch: 98, loss: 0.02733972668647766\n",
      "epoch: 99, loss: 0.06540576368570328\n"
     ]
    }
   ],
   "source": [
    "model = LinearRewardNet().to(device)\n",
    "\n",
    "critrion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(data_norm)\n",
    "    loss = critrion(y_pred, model.profit_target)\n",
    "    print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
